step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
bike_preg <- linear_reg(penalty=tune(),
mixture=tune()) |>
set_engine("glmnet")
bike_preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_preg)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(bike_train, v=10)
CV_results <- bike_preg_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse))
bestTune <- CV_results |>
select_best(metric="rmse")
final_wf <- bike_preg_wf |>
finalize_workflow(bestTune) |>
fit(data=bike_train)
lin_preds <- final_wf |>
predict(new_data = bike_test)
kaggle_submission <- lin_preds |>
bind_cols(bike_test) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)),
count=exp(count))
vroom_write(x=kaggle_submission, file="./Tune_Pen_Preds.csv", delim = ",")
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_recipe <- recipe(count~., data = bike_train) |>
step_mutate(
season=factor(season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")),
weather=as.factor(ifelse(weather==4, 3, weather)),
holiday=as.factor(holiday),
workingday=as.factor(workingday)) |>
step_date(datetime, features = c("month", "dow")) |>
step_time(datetime, features = "hour", keep_original_cols = F) |>
step_mutate(datetime_hour=as.factor(datetime_hour),
datetime_month=as.factor(datetime_month),
datetime_dow=as.factor(datetime_dow)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
bike_tree <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) |>
set_engine("rpart") |>
set_mode("regression")
bike_tree_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_tree)
grid_of_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
folds <- vfold_cv(bike_train, v=5)
CV_results <- bike_tree_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse))
bestTune <- CV_results |>
select_best(metric="rmse")
final_wf <- bike_tree_wf |>
finalize_workflow(bestTune) |>
fit(data=bike_train)
tree_preds <- final_wf |>
predict(new_data = bike_test)
kaggle_submission <- tree_preds |>
bind_cols(bike_test) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)),
count=exp(count))
vroom_write(x=kaggle_submission, file="./Tree_Preds.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(skimr)
library(DataExplorer)
library(GGally)
library(glmnet)
library(rpart)
library(ranger)
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_recipe <- recipe(count~., data = bike_train) |>
step_mutate(
season=factor(season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")),
weather=as.factor(ifelse(weather==4, 3, weather)),
holiday=as.factor(holiday),
workingday=as.factor(workingday)) |>
step_date(datetime, features = c("month", "dow")) |>
step_time(datetime, features = "hour", keep_original_cols = F) |>
step_mutate(datetime_hour=as.factor(datetime_hour),
datetime_month=as.factor(datetime_month),
datetime_dow=as.factor(datetime_dow)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
bike_forest <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 300) |>
set_engine("ranger") |>
set_mode("regression")
bike_forest_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_forest)
grid_of_tuning_params_forest <- grid_regular(
mtry(range = c(1, ncol(bake(prep(bike_recipe), bike_train)))),
min_n(),
levels = 5)
folds <- vfold_cv(bike_train, v=5)
CV_results <- bike_forest_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params_forest,
metrics=metric_set(rmse))
bestTune <- CV_results |>
select_best(metric="rmse")
final_wf <- bike_forest_wf |>
finalize_workflow(bestTune) |>
fit(data=bike_train)
forest_preds <- final_wf |>
predict(new_data = bike_test)
kaggle_submission <- forest_preds |>
bind_cols(bike_test) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)),
count=exp(count))
vroom_write(x=kaggle_submission, file="./Forest_Preds.csv", delim = ",")
install.packages("stacks")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(skimr)
library(DataExplorer)
library(GGally)
library(glmnet)
library(rpart)
library(ranger)
library(stacks)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(skimr)
library(DataExplorer)
library(GGally)
library(glmnet)
library(rpart)
library(ranger)
library(stacks)
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_recipe <- recipe(count~., data = bike_train) |>
step_mutate(
season=factor(season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")),
weather=as.factor(ifelse(weather==4, 3, weather)),
holiday=as.factor(holiday),
workingday=as.factor(workingday)) |>
step_date(datetime, features = c("month", "dow")) |>
step_time(datetime, features = "hour", keep_original_cols = F) |>
step_mutate(datetime_hour=as.factor(datetime_hour),
datetime_month=as.factor(datetime_month),
datetime_dow=as.factor(datetime_dow)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
install.packages("parallel")
install.packages("parallel")
install.packages("parallel")
install.packages("parallel")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(skimr)
library(DataExplorer)
library(GGally)
library(glmnet)
library(rpart)
library(ranger)
library(stacks)
library(parallel)
detectCores()
return(CV_results)
run_cv <- function(){
cl <- makePSOCKcluster(8)
doParallel::registerDoParallel(cl)
cvStart <- Sys.time()
CV_results <- bike_forest_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params_forest,
metrics=metric_set(rmse))
print("CV time: ")
Sys.time()-cvStart
stopCluster(cl)
return(CV_results)
}
CV_results = run_cv()
run_cv <- function(){
cl <- makePSOCKcluster(8)
doParallel::registerDoParallel(cl)
cvStart <- Sys.time()
CV_results <- bike_forest_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params_forest,
metrics=metric_set(rmse))
print("CV time: ")
Sys.time()-cvStart
stopCluster(cl)
return(CV_results)
}
CV_results = run_cv()
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_recipe <- recipe(count~., data = bike_train) |>
step_mutate(
season=factor(season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")),
weather=as.factor(ifelse(weather==4, 3, weather)),
holiday=as.factor(holiday),
workingday=as.factor(workingday)) |>
step_date(datetime, features = c("month", "dow")) |>
step_time(datetime, features = "hour", keep_original_cols = F) |>
step_mutate(datetime_hour=as.factor(datetime_hour),
datetime_month=as.factor(datetime_month),
datetime_dow=as.factor(datetime_dow)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
bike_forest <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 300) |>
set_engine("ranger") |>
set_mode("regression")
bike_forest_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_forest)
grid_of_tuning_params_forest <- grid_regular(
mtry(range = c(1, ncol(juice(prep(bike_recipe), bike_train)))),
min_n(),
levels = 5)
grid_of_tuning_params_forest <- grid_regular(
mtry(range = c(1, ncol(juice(prep(bike_recipe)), bike_train))),
min_n(),
levels = 5)
grid_of_tuning_params_forest <- grid_regular(
mtry(range = c(1, ncol(juice(prep(bike_recipe))), bike_train)),
min_n(),
levels = 5)
grid_of_tuning_params_forest <- grid_regular(
mtry(range = c(1, ncol(juice(prep(bike_recipe))))),
min_n(),
levels = 5)
folds <- vfold_cv(bike_train, v=5)
run_cv <- function(){
cl <- makePSOCKcluster(8)
doParallel::registerDoParallel(cl)
cvStart <- Sys.time()
CV_results <- bike_forest_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params_forest,
metrics=metric_set(rmse))
print("CV time: ")
Sys.time()-cvStart
stopCluster(cl)
return(CV_results)
}
CV_results = run_cv()
bestTune <- CV_results |>
select_best(metric="rmse")
besttune
bestTune
bike_recipe <- recipe(count~., data = bike_train) |>
step_mutate(
season=factor(season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")),
weather=as.factor(ifelse(weather==4, 3, weather)),
holiday=as.factor(holiday),
workingday=as.factor(workingday)) |>
step_date(datetime, features = c("month", "dow")) |>
step_time(datetime, features = "hour", keep_original_cols = F) |>
step_mutate(datetime_hour=as.factor(datetime_hour),
datetime_month=as.factor(datetime_month),
datetime_dow=as.factor(datetime_dow)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_predictors())
folds <- vfold_cv(bike_train, v = 5)
untunedModel <- control_stack_grid()
tunedModel <- control_stack_resamples()
bike_preg <- linear_reg(penalty=tune(),
mixture=tune()) |>
set_engine("glmnet")
bike_preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_preg)
tuning_grid_preg <- grid_regular(penalty(),
mixture(),
levels = 5)
preg_models <- bike_preg_wf |>
tune_grid(resamples=folds,
grid=tuning_grid_preg,
metrics=metric_set(rmse, mae, rsq),
control=untunedModel)
stopCluster(cl)
preg_models <- bike_preg_wf |>
tune_grid(resamples=folds,
grid=tuning_grid_preg,
metrics=metric_set(rmse),
control=untunedModel)
unregister_dopar <- function() {
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
unregister_dopar()
preg_models <- bike_preg_wf |>
tune_grid(resamples=folds,
grid=tuning_grid_preg,
metrics=metric_set(rmse),
control=untunedModel)
bike_lin_reg <- linear_reg() |>
set_engine("lm")
lin_reg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_lin_reg)
lin_reg_model <- fit_resamples(lin_reg_wf,
resamples=folds,
metrics=metric_set(rmse),
control=tunedModel)
bike_forest <- rand_forest(mtry = 52,
min_n = 11,
trees = 300) |>
set_engine("ranger") |>
set_mode("regression")
bike_forest_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(bike_forest)
tuning_grid_forest <- grid_regular(
mtry(range = c(1, ncol(bake(prep(bike_recipe)-1, bike_train)))),
min_n(),
levels = 5)
tuning_grid_forest <- grid_regular(
mtry(range = c(1, ncol(juice(prep(bike_recipe))))-1),
min_n(),
levels = 5)
rforest_model <- bike_forest_wf |>
fit_resamples(resamples=folds,
grid=tuning_grid_forest,
metrics=metric_set(rmse),
control=tunedModel)
my_stack <- stacks() |>
add_candidates(rforest_model) |>
add_candidates(lin_reg_model) |>
add_candidates(preg_models)
stack_model <- my_stack |>
blend_predictions() |>
fit_members()
stack_model |> predict(new_data=bike_test)
stack_model
stack_preds <- stack_model |> predict(new_data=bike_test)
kaggle_submission <- stack_preds |>
bind_cols(bike_test) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)),
count=exp(count))
vroom_write(x=kaggle_submission, file="./Stack_Preds.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(skimr)
library(DataExplorer)
library(GGally)
library(glmnet)
library(rpart)
library(ranger)
library(stacks)
library(parallel)
install.packages("dbart")
?dbart
??dbart
install.packages("parsnip")
source(bike_recipe)
source(bike_recipe.R)
source("bike_recipe.R")
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_recipe(bike_train)
bike_rec <- bike_recipe(bike_train)
source("bike_recipe.R")
source("bike_recipe.R")
CV_results <- run_cv(folds, grid, rmse)
bike_train <- vroom("train.csv")
bike_train <- bike_train  |>
select(-casual, -registered) |>
mutate(count=log(count))
bike_test <- vroom("test.csv")
bike_rec <- bike_recipe(bike_train)
bike_bart <- rand_bart(trees = 300) |>
set_engine("dbart") |>
set_mode("classification")
bike_bart <- bart(trees = 300) |>
set_engine("dbart") |>
set_mode("classification")
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("classification")
bike_bart_wf <- workflow() |>
add_recipe(bike_rec) |>
add_model(bike_bart)
bike_bart_wf <- workflow() |>
add_recipe(bike_rec) |>
add_model(bike_bart) |>
fit(data=training_data)
bike_bart_wf <- workflow() |>
add_recipe(bike_rec) |>
add_model(bike_bart) |>
fit(data=bike_train)
install.packages("dbarts")
library(dbarts)
final_wf <- workflow() |>
add_recipe(bike_rec) |>
add_model(bike_bart) |>
fit(data=bike_train)
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression")
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression")
regression
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression")
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression")
?bart
bike_bart <- bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression") |>
translate()
bike_bart <- bart(trees = NULL) |>
set_engine("dbarts") |>
set_mode("regression")
bike_bart <- bart() |>
set_engine("dbarts") |>
set_mode("regression")
bike_bart <- parsnip::bart() |>
set_engine("dbarts") |>
set_mode("regression")
bike_bart <- parsnip::bart(trees = 300) |>
set_engine("dbarts") |>
set_mode("regression")
final_wf <- workflow() |>
add_recipe(bike_rec) |>
add_model(bike_bart) |>
fit(data=bike_train)
bart_preds <- final_wf |>
predict(new_data = bike_test)
bart_preds
kaggle_submission <- bart_preds |>
bind_cols(bike_test) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)),
count=exp(count))
vroom_write(x=kaggle_submission, file="./Bart_Preds.csv", delim = ",")
nrow(bart_preds)
(pi/2)(sin(pi/4))+(pi/4)sin(pi/2)
(pi/2)*(sin(pi/4))+(pi/4)*sin(pi/2)
pi*sqrt(2)/2
sin(pi/4)
sqrt(2)/2
(pi/2)*sin(p/4)
(pi/2)*sin(pi/4)
pi*sqrt(2)/4
sin(pi/2)
(pi/4)*sin(pi/2)
pi/4
(pi*sqrt(2)+pi)/4
